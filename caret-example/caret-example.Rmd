---
title: "A Short Machine Learning Essay using the Caret Library"
output:
  html_document:
    df_print: paged
---

From the [Caret Bookdown by Max Kuhn](https://topepo.github.io/caret/index.html): 

>The caret package (short for Classification And REgression Training) is a set of functions that attempt to streamline the process for creating predictive models. The package contains tools for:

* data splitting
* pre-processing
* feature selection
* model tuning using resampling
* variable importance estimation

>as well as other functionality.

[Data Analysis](/Users/aaronmamula/Documents/R projects/rad-teaching-stuff/caret-example/figures/xkcd_comic.png)

# Introduction

In this notebook I will use R's [caret](http://caret.r-forge.r-project.org/) library to train two predictive models: an Artificial Neural Network and a Gradient Boosted Classification Tree.  The empirical setting is the following: I have locational data on fishing vessels from satellite tracking system logged at approximately hourly time intervals (polls) and I wish to fit a model capable of classifying an unlabeled observation according to whether the vessel was fishing or not fishing at the time of the poll.

The primary purpose of this notebook is:

to illustrate the usage of caret's train() method for training predictive models

The secondary objectives is:

to highlight how functions in the caret library can be combined to establish a machine learning pipeline including:

1. pre-processing data (normalizing continuous variables, one-hot encoding)
2. dealing with NAs or outliers
3. examining final models visually

[Data Pipeline](/Users/aaronmamula/Documents/R projects/rad-teaching-stuff/caret-example/figures/datapipeline.png)

## Dependencies

* dplyr - for basic data manipulation
* data.table - for list-to-data frame operations
* ggplot2 - visuals
* lubridate - dealing with date-times
* nnet - neural networks
* NeuralNetTools - for plotting nnet objects
* gbm - stochastic gradient boosting
* caret - training predictive models

```{r echo=F,warnings=F}
rm(list=ls())
library(dplyr)
library(data.table)
library(ggplot2)
library(caret)
library(nnet)
library(lubridate)
library(gbm)
library(NeuralNetTools)
```

# Empirical Setting

These data come from a satellite monitoring system that tracks the location of commercial fishing vessels in the U.S.  These data are logged on hourly intervals and record a vessel's location in lat/long coordinates and provide a time stamp.  

In a separate process I have joined these data with a bathymetry grid to get the ocean bottom depth associated with each set of lat/long coordinates.  I have also joined these data with a secondary data source containing captains logs recorded during fishing operations.  By joining the positional data with the captain's logs I was able to create a variable indicating whether the vessel was actively fishing during the time the observation was taken.

I have also used the lat/long values and time-stamps to calculate derived fields for vessel speed and bearing in radians.

Before uploading the data to GitHub I removed the actual lat/long values and vessel identifiers to protect confidential information.  

```{r}
df <- tbl_df(read.csv('fishing_data.csv')) %>%
       mutate(utc_date=as.POSIXct(utc_date,format="%Y-%m-%d %H:%M:%S",tz='UTC'),
              local_time=as.POSIXct(local_time,format="%Y-%m-%d %H:%M:%S"))
str(df)
```

One quick change I'm going to make here is to make the target variable a factor rather than [0,1]...many of the ML libraries in R don't like numeric target variable for two-class prediction problems.

```{r}
df <- df %>% mutate(y=factor(ifelse(fishing==0,'notfishing','fishing')))
levels(df$y)
```
The object of the empirical exercise is to train a model capable of classifying an unlabeled observation as either "fishing" or "not fishing".  The features we have available for our modeling exercise are:

* Outcome Variable/Target Variable
    + fishing - A [0,1] variable indicating whether the vessel was fishing or not  

* Inputs
    + len - vessel length in feet
    + boat - a unique vessel identifier
    + hour - the hour of the day
    + bearing.rad - bearing in radians
    + speed - instantaneous speed
    + bottom_depth - ocean bottom depth
  
# The Models

I don't intend to discuss the particulars of ANNs and Gradient Boosting here.  I'm assuming a decent level of familiarity with these models and concepts so I can focus on the details of using the [caret](http://topepo.github.io/caret/index.html) library to train the models.  Check out the links below to dive into the models:

## ANNs

[Watch this video](https://www.youtube.com/watch?v=Ilg3gGewQ5U) to get a really cool look at optimizing a neural network using the backpropagation algorithm.

The cononical resource for all things machine learning [Hastie, Tibshirani, and Freidman](https://web.stanford.edu/~hastie/Papers/ESLII.pdf), Chapter 11.

I also like [this ebook specifically on Neural Networks](http://www.dkriesel.com/_media/science/neuronalenetze-en-zeta2-2col-dkrieselcom.pdf) by David Kriesel.

## GBM

Again, a great place to start is [Hastie, Tibshirani, and Freidman](https://web.stanford.edu/~hastie/Papers/ESLII.pdf). Gradient Boosting isn't as well contained as Neural Networks and the concepts kind of pops up in a lot of places.  Chapter 10 is a pretty good place to start for readers interested in gradient boosting trees.


[Here are some slides](https://homes.cs.washington.edu/~tqchen/pdf/BoostedTree.pdf) from a Computer Science Lecture at U. Washington that I think give a nice overview and reasonably rigorous presentation of GBM.

[Here is a neat hands-on](http://uc-r.github.io/gbm_regression) vignette. 

# Training the Models with CARET

For this example I'm going to do a couple things outside of the caret environment:

1. make the vessel length and hour-of-day variables categorical.  I don't have a great reason for this, I just want some factor-type variables in my input space to make some points about caret.

2. I'm going to cut down the data a bit so training the models doesn't take forever.

Full disclosure: I think there should be a way to do the continuous-to-categorical variable tranformation within caret...but, if there is, I haven't figured it out yet.

```{r}
# just gonna make some of the continuous variables categorical
df <- df %>% ungroup() %>% mutate(len=as.factor(as.numeric(cut(len,5))),
                                    hour=as.factor(as.numeric(cut(hour,4))))


# make the data set smaller so we can deal with it
df <- df %>% filter(year(local_time) == 2014 & month(local_time) %in% c(7,8,9)) %>% 
  select(y,fishing,speed,bottom_depth,len,hour,bearing.rad) %>%
  select(-fishing)

str(df)
```


## Splitting Data

The caret library has a *createDataPartition()* method that can split your data into training and testing partitions.  By default, the sampling scheme used by *createDataPartition()* will attempt to balance the class distribution of the target variable among splits.  [From the documentation](https://topepo.github.io/caret/data-splitting.html)

> The function createDataPartition can be used to create balanced splits of the data. If the y argument to this function is a factor, the random sampling occurs within each class and should preserve the overall class distribution of the data. 

```{r}
#--------------------------------------------------------------------
#create the training and testing data
index <- createDataPartition(df$y,p=0.8,list=F,times=1)
df.train <- df[index,]
df.test <- df[-index,]  
#--------------------------------------------------------------------

str(df.train)
str(df.test)

levels(df.train$y)
```

Let's have a quick look at the target variables in the training and testing sets:

```{r}
df.train %>% group_by(y) %>% summarise(count=n())
df.test %>% group_by(y) %>% summarise(count=n())

```

"Fishing" observations are in the neighborhood of 20-25% of the training and testing data...so that's cool.

The caret library is pretty flexible and has some cool options for other sampling schemes if balancing class distributions isn't your thing:

* a function *maxDissim()* can create data splits based on maximum dissimilarity
* a function *createTimeSlices()* can create indexes for splitting time-series data

Finally, if you prefer to roll your own K-fold Cross-Validation, there is a function *createFolds()* that will generate index values to use in assigning observations to folds:

```{r}
# create a column assigning each observation to one of 5 folds for CV:
df$fold <- createFolds(df$y,k=5,list=F)
head(df)
df <- df %>% select(-fold)
```


## Preprocessing

I'm going to do a pretty mundane and popular data transformation here: I'm going to normalize my continuous variables (speed, bottom_depth, and bearing) so that lie on the [0,1] interval.  This is a common processing step done in machine learning to avoid scale bias.  Noteably however, I want to leave my categorical variables alone.  

Caret has a *preProcess()* function for this.  It works like this:

```{r}
# pre process the data....only the continuous variables
preProcValues <- preProcess(df.train, method = c("range"))
trainTransformed <- predict(preProcValues, df.train)
preProctest <- preProcess(df.test,method=c("range"))
testTransformed <- predict(preProctest, df.test)

preProcValues
```

The output here tells us that 3 categorical variables were ignored and 3 continuous variables were normalized.  

```{r}
head(trainTransformed)
```

## Setting hyper-parameter grids

In our *Splitting the Data* section we reviewed how training data could be split into folds for Cross-Validation using *createFolds()*.  In practice, caret has a method called *train()* that can be used to perform different flavors of cross-validation.

### training method

Several training methods are supported including: 

* cv - K-fold cross validation
* repeatedcv - repeated K-fold cross validation
* LOOCV - leave one out cross validation
* oob - out-of-bag estimates for use with random forest models

[Section 5.5.4](http://topepo.github.io/caret/model-training-and-tuning.html#custom) details the different options that can be declared for ```train(method=...)```.

### summary function

[Here's a primer on ROC curves for evaluating ML Models](https://medium.com/greyatom/lets-learn-about-auc-roc-curve-4a94b4d88152).

The actual summary function that one uses to evaluate the 'best' model will be declared within the *train()* method that gets called later.  But *train()* only has access to model summaries output by the ML algorithm being called.

Caret provides a function to calculate alternative performance metrics:

*summaryFunction=* 

which can be used to add more user control over the performance metrics.  

In the code below I specify ```summaryFunction=twoClassSummary ```.  This is because I am using the [gbm library](https://cran.r-project.org/web/packages/gbm/gbm.pdf) to train the gradient boosted tree.  The *gbm()* function does not compute an ROC metric by default.  So if we want to use ROC to evaluate the gbm models, we need to add that functionality using caret's *summaryFunction* option.

```{r}
# set up control parameters for the neural network
fitControl <- trainControl(method = "cv", 
                           number = 5, 
                           classProbs = TRUE, 
                           summaryFunction = twoClassSummary)

# make the hyper-parameter grid pretty small so it doesn't run forever
nnetGrid <-  expand.grid(size = seq(from = 5, to = 8, by = 1),
                         decay = seq(from = 0.1, to = 0.2, by = 0.1))

# set up control parameters for the GBM
fitControl.GBM <- trainControl(method = "cv", 
                               number = 5,
                               classProbs = TRUE,
                               summaryFunction = twoClassSummary)

```

## Training

The real red meat of the caret library is the *train()* function.  *train()* works with [238 models](http://topepo.github.io/caret/available-models.html) from a variety of different R libraries.  

### The ANN

In the chunk below we use the preprocessed training data to train a neural network model using the [nnet](https://cran.r-project.org/web/packages/nnet/nnet.pdf) library.

The model is trained using 5 fold Cross-Validation which was declared in the *fitControl* object created by caret's *trainControl()* function.  The hyper-parameters used for training are defined in the *tuneGrid()* argument.

```{r}
#----------------------------------------------------------------------
# Train the models
t <- Sys.time()
nnetFit <- train(y ~ ., 
                 data = trainTransformed,
                 method = "nnet",
                 metric = "ROC",
                 trControl = fitControl,
                 tuneGrid = nnetGrid,
                 na.action=na.omit,
                 trace=F,
                 verbose = FALSE)
Sys.time() - t
nnetFit$finalModel
```

Here's something worth noticing (maybe it matters a lot maybe it doesn't matter at all): The ```nnet()``` method actually does a little behind-the-scenes variable transformation.  Specifically, it takes the categorical variables *len* and *hour* and uses [one hot encoding](https://en.wikipedia.org/wiki/One-hot) to create a set of binary variables.

Just to further illuminate the crucial relationship here, I'm going to use the *nnet()* function from the [nnet](https://cran.r-project.org/web/packages/nnet/nnet.pdf) package directly and compare.

```{r}
nn2 <- nnet(y~.,data=trainTransformed,size=5,linout=F,na.action=na.omit,trace=F)
nn2
```

For illustrative purposes, just look at the list objects returned by train() and nnet():

```{r}
names(nnetFit$finalModel)

```


```{r}
names(nn2)
```

To restate the obvious:

* nnetFit is a list object returned by the *train()* function which performed a grid search over multiple values of 2 hyper-parameters (size and decay) to find the "best" single hidden layer neural network model based on 5 Fold Cross-Validation using ROC as the selection criteria.

* nn2 is list object containing the results of a single call to the *nnet()* function


### GBM

```{r}

gbmGrid <-  expand.grid(interaction.depth = c(1, 5, 9), 
                        n.trees = (1:30)*50, 
                        shrinkage = 0.1,
                        n.minobsinnode = 20)
t <- Sys.time()
# tree-based methods generally don't need input scaling
gbmFit <- train(y ~ ., 
                #data = trainTransformed,
                data = df.train,
                method = "gbm",
                metric = "ROC",
                trControl = fitControl.GBM,
                tuneGrid = gbmGrid,
                na.action = na.omit,
                verbose = FALSE)
Sys.time() - t
#-------------------------------------------------------------------------

```

See what the train()-gbm() output looks like:

```{r}
names(gbmFit)
```


```{r}
names(gbmFit$finalModel)
```


# Prediction

Using the optimal models - where optimal is defined as the best model in each class conditional on our tuning parameters - we can generate predicted values for the observations in the testing data.  This is done pretty simply using [predict.train()](https://www.rdocumentation.org/packages/caret/versions/5.05.004/topics/predict.train) method, which provides a wrapper to prediction functions associated with each model.

```{r}
testTransformed <- testTransformed[complete.cases(testTransformed),]
df.test <- df.test[complete.cases(df.test),]
#-------------------------------------------------------------------------
# make predictions
NNpredictions <- predict(nnetFit,testTransformed)
GBMpredictions <- predict(gbmFit,df.test)

#--------------------------------------------------------------------------
```


This probably isn't a huge deal but note that we washed out the NA values from the testing data before calling to the predict() function.  We could have added an ```na.action=na.omit``` option to the ```predict()``` call but, honestly, this seems to me to just complicate thing unnecessarily.  What I mean by that is this:

```predict(nnetmodel,newdata=,na.action=na.omit)``` is going to return a vector of predictions with size ```nrow(testTransformed) - nrow(testTransformed[complete.cases(testTransformed),])```.  To get the confusion matrix, which we want in order to evaluate the two model classes (neural network v. GBM), we need to line up observed outcomes from the testing data with the prediction.  I guess we could just index the testing data so that ```nrow(testTransformed)=length(NNpredictions)```.  But that doesn't strike me as any more elegant or sensible than just removing NAs from the data before calling ```predict()```.


```{r}

Net_CM <- confusionMatrix(NNpredictions,testTransformed$y)
GBM_CM <- confusionMatrix(GBMpredictions,df.test$y)

Net_CM
GBM_CM
```


```{r}
#f1 score for the ANN
p <- Net_CM$table[2,2]/(Net_CM$table[2,2] + Net_CM$table[2,1])
r <- Net_CM$table[2,2]/(Net_CM$table[2,2] + Net_CM$table[1,2])
(2*p*r)/(p+r)

#f1 score for the GBM
p <- GBM_CM$table[2,2]/(GBM_CM$table[2,2] + GBM_CM$table[2,1])
r <- GBM_CM$table[2,2]/(GBM_CM$table[2,2] + GBM_CM$table[1,2])
(2*p*r)/(p+r)

```


# Some Fun Post-Processing Visualization

There are plenty of model diagnostic methods available in caret.  [See Chapters 15 and 18-22](https://topepo.github.io/caret/variable-importance.html).  I'm just going to do a quick demo of two:

1. plotting variable importance from the GBM models, and
2. visualizing the optimal ANN using the [plotnet add-on](https://www.rdocumentation.org/packages/NeuralNetTools/versions/1.5.2/topics/plotnet)

The [gbm library](https://cran.r-project.org/web/packages/gbm/gbm.pdf) has built-in variable importance methods which can be used to visualize some interesting features of the optimal gbm model:

```{r}
vars <- varImp(gbmFit, scale = FALSE)
plot(vars)
```


```{r}
plotnet(nnetFit)
```




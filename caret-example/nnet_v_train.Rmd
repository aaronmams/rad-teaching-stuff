---
title: "A closer look at nnet() and caret::train()"
output:
  html_document:
    df_print: paged
---

The purpose of this workbook is to enhance understanding of the caret library by looking at two ways to estimate a neural network that should be roughly equivilent.  The first approach uses caret to train a neural network using [nnet::nnet()](https://cran.r-project.org/web/packages/nnet/nnet.pdf).  The second uses the *nnet()* function directly.   

# Dependencies

```{r warnings=F}
rm(list=ls())
library(dplyr)
library(data.table)
library(ggplot2)
library(caret)
library(nnet)
library(lubridate)
library(gbm)
library(NeuralNetTools)
library(AUC)

```

# Data Set Up

Start with the same date that we used on the [caret-example.RMD](https://github.com/aaronmams/rad-teaching-stuff/blob/master/caret-example/caret-example.Rmd) notebook.

Note that I'm doing a couple things here:

1. partitioning the data into training and test...not super important for this experiment but it cuts down on run time
2. preprocess the data by normalizing all continuous input variables

```{r}
df <- tbl_df(read.csv('fishing_data.csv')) %>%
       mutate(utc_date=as.POSIXct(utc_date,format="%Y-%m-%d %H:%M:%S",tz='UTC'),
              local_time=as.POSIXct(local_time,format="%Y-%m-%d %H:%M:%S"))
df <- df %>% mutate(y=factor(ifelse(fishing==0,'notfishing','fishing')))
df <- df %>% ungroup() %>% mutate(len=as.factor(as.numeric(cut(len,5))),
                                    hour=as.factor(as.numeric(cut(hour,4))))

# make the data set smaller so we can deal with it
df <- df %>% filter(year(local_time) == 2014 & month(local_time) %in% c(7,8,9)) %>% 
  select(y,fishing,speed,bottom_depth,len,hour,bearing.rad) %>%
  select(-fishing)

#make a smaller partition
index <- createDataPartition(df$y,p=0.5,list=F,times=1)
df.train <- df[index,]
df.test <- df[-index,]  

# pre process the data....only the continuous variables
preProcValues <- preProcess(df.train, method = c("range"))
trainTransformed <- predict(preProcValues, df.train)
testTransformed <- predict(preProcValues, df.test)
```


# An Apples-to-Apples Comparison

Here is as close as I could come to an apples-to-apples comparison of a 5 Fold CV experiment executed using two approaches illustrated using pseudo-code:

* Set up
    + use createDataPartition() to create a managable data set that perserves target variable distribution
    
* Method 1
    + caret::fitControl() to define cross validation process
    + caret::expandGrid() to set hyper parameters
    + caret::train(method='nnet'...)
    
* Method 2
    + get CV folds used in Method 1
    + train using nnet() on all folds except testing fold
    + predict outcomes in testing fold
    + calculate ROC using caret::twoClassSummary()

Here is the step-by-step of what I did:

1. define a single set of hyper-parameters (size = 5, decay = 0.1, maxit = 1000)
2. run a 5 Fold CV using caret::train()
3. get the row indexes for the CV folds used by caret::train()
4. get the ROC values computed for each fold
5. partition the data by hand into the same folds used by caret::train()
6. train the model using nnet() for each fold
7. for each fold, predict the target variable using predict() on the nnet object
8. use the convenience function caret::twoClassSummary() to compute ROC for each fold

Here is how I coded it.  Note, I'm coding this as a function because I'm going to run it a few times and illustrate how, sometimes I get the same ROC from the two approaches...and sometimes I don't.  I guess we can discuss as a group how worrisome that fact should be.


```{r}
#######################################################################
#######################################################################
# compare ROC for train() versus nnet()

#----------------------------------------------------------------

comp.fn <- function(data){
df <- data  

idx <- createDataPartition(df$y,p=0.005,list=F,times=1)
df <- df[idx,]

# set up control parameters for the neural network
fitControl <- trainControl(method = "cv", 
                           number = 5, 
                           classProbs = TRUE, 
                           summaryFunction = twoClassSummary)

# make the hyper-parameter grid pretty small so it doesn't run forever
nnetGrid <-  expand.grid(size = seq(from = 5, to = 5, by = 1),
                         decay = seq(from = 0.1, to = 0.1, by = 0.1))

nnetFit <- train(y ~ speed + bottom_depth + len + hour + bearing.rad, 
                 data = df,
                 method = "nnet",
                 maxit= 1000,
                 metric = "ROC",
                 trControl = fitControl,
                 tuneGrid = nnetGrid,
                 na.action=na.omit,
                 trace=F,
                 verbose = FALSE)

nnetFit$resample
#----------------------------------------------------------------------

#----------------------------------------------------------------------
# make the data frame look a certain way because twoClassSummary() is 
# kind of fickle
df.comp <- df %>% mutate(obs=factor(ifelse(y=='fishing','class1','class2'))) %>% select(-y)
head(df.comp)

# start with the same CV folds that are in train
index <- nnetFit$control$index

roc <- list()
for(i in 1:5){

train <- df.comp[index[[i]],]
test <- df.comp[-index[[i]],]
res <- nnet(obs~speed+bottom_depth+len+hour+bearing.rad,
            data=train,size=5,decay=0.1,maxit=1000,lineout=F,na.action=na.omit,trace=F)

test$pred <- predict(res,test,type="class")
test$classprob <- predict(res,test,type="raw")


dat.sum <- data.frame(pred=test$pred,
                      obs=test$obs,
                      class1=1-test$classprob,
                      class2=test$classprob)

#dat.sum <- dat.sum[complete.cases(dat.sum),]
classes = c('class1','class2')

roc[[i]] <- twoClassSummary(dat.sum, lev = classes)[1]
}


#format output a little
roc.net <- data.frame(fold=c('Fold1','Fold2','Fold3','Fold4','Fold5'),ROC=unlist(roc),method="nnet")
roc.caret <- data.frame(fold=nnetFit$resample$Resample,ROC=nnetFit$resample$ROC,method='caret')
result <- tbl_df(data.frame(rbind(roc.caret,roc.net))) %>% arrange(fold)
return(result)
}
```


```{r}
comp.fn(data=trainTransformed[complete.cases(trainTransformed),])
```

```{r}
comp.fn(data=trainTransformed[complete.cases(trainTransformed),])
```

```{r}
comp.fn(data=trainTransformed[complete.cases(trainTransformed),])
```


I don't know exactly where I'm going with this...other than to say, I was a little bummed that I went to all the trouble of doing this a few different ways in order to really understand the process only to find out that I sometimes get different answers.  I guess this isn't a huge deal as long as the model selection is reasonably consistent.  Which brings me to my next section:


# Is Model Selection Consistent?

Here, I'm going to do 5 Fold CV to choose the best ANN conditional on the following hyper parameter grid:

```{r}
nnetGrid <-  expand.grid(size = seq(from = 5, to = 10, by = 1),
                         decay = seq(from = 0.1, to = 0.5, by = 0.1))

```


I'm going to do the 5 Fold CV 3 ways:

1.  caret::train()
2.  nnet() by hand using the caret::twoClassSummary() function to compute ROC
3.  nnet() by hand using AUC::auc(roc()) to compute ROC

Approaches #2 and #3 will rank models based on ROC average across the 5 folds.  I assume that caret::train() also uses some kind of score averaging across CV folds to compute a model performance metric. 

First, make another manageable dataset from the larger data set.  I'm going to simplify this example by removing all the NA values before hand.

```{r}
df.comp <- trainTransformed[complete.cases(trainTransformed),]
idx <- createDataPartition(df.comp$y,p=0.1,list=F,times=1)
df.comp <- df.comp[idx,]
nrow(df.comp)
df.comp %>% group_by(y) %>% summarise(count=n())
```


## Method 1

This is the pretty straight forward method utilizing caret.

```{r}
t <- Sys.time()
# set up control parameters for the neural network
fitControl <- trainControl(method = "cv", 
                           number = 5, 
                           classProbs = TRUE, 
                           summaryFunction = twoClassSummary)


mod1 <- train(y ~ speed + bottom_depth + len + hour + bearing.rad, 
                 data = df.comp,
                 method = "nnet",
                 maxit= 1000,
                 metric = "ROC",
                 trControl = fitControl,
                 tuneGrid = nnetGrid,
                 na.action=na.omit,
                 trace=F,
                 verbose = FALSE)

Sys.time() - t
```


## Method 2 and 3

Method 2 is the method where we invoke the following pipeline:

>caret::createFolds() --> set hyper-params --> nnet() --> caret::twoClassSummary()

Method 3 is the 'mostly' caret independent process:

>caret::createFolds() --> set hyper-params --> nnet() --> AUC::auc(roc())
 
Note that Method 3 is nested within the code for model 2 as the only thing that is really different between them is that Method 2 uses caret::twoClassSummary() to compute ROC and Method 3 uses AUC::auc(roc()) to compute ROC. 

```{r}
t <- Sys.time()


    # create CV folds by hand
    df.comp$fold <- createFolds(df.comp$y,k=5,list=F)
    df.comp <- df.comp %>% mutate(obs=factor(ifelse(y=='fishing','class1','class2'))) %>% select(-y)
    # set up a list to collect ROC values
    roc <- list()

    for(iparam in 1:nrow(nnetGrid)){
      size.now <- nnetGrid$size[iparam]
      decay.now <- nnetGrid$decay[iparam]
      
      roc.fold <- list()
      for(k in 1:5){
        
        train <- df.comp[df.comp$fold!=k,]
        test <- df.comp[df.comp$fold==k,]
        mod <- nnet(obs ~ speed + bottom_depth + len + hour + bearing.rad, 
                    data = train,
                    size=size.now,
                    decay=decay.now,
                    maxit=1000,
                    lineout=F,
                    trace=F)
        
        test$pred <- predict(mod,test,type="class")
        test$classprob <- predict(mod,test,type="raw")
#------------------------------------------------------------------        
# massage some stuff to be able to use the twoClassSummary() function
        
        tmp <- data.frame(pred=test$pred,
                          obs=test$obs,
                          class1=1-test$classprob,
                          class2=test$classprob)
        
        classes = c('class1','class2')
        
        roc.tmp1 <- twoClassSummary(tmp, lev = classes)[1]
#----------------------------------------------------------------

      # Now just get ROC from the AUC library         
      roc.tmp2<- AUC::auc(roc(test$pred,test$obs))
      roc.fold[[k]] <- data.frame(size=size.now,decay=decay.now,ROC=c(roc.tmp1,roc.tmp2),
                                  method=c('ROCcaret','ROC-AUC'))  
      }
      roc[[iparam]] <- data.frame(rbindlist(roc.fold))      
    }
Sys.time() - t
    
```

We can view the optimal networks from approaches 2 and 3 by manipulating the list result from the code chunk above:

```{r}
roc <- tbl_df(data.frame(rbindlist(roc))) %>% group_by(size,decay,method) %>%  
           summarise(ROC=mean(ROC)) %>% arrange(method,-ROC)
roc         
```

We can view the optimal model from approach 1 by:

```{r}
mod1
```


I don't really have any grand point I'm trying to make here.  I just thought it was interesting and worth emphasizing that:

Even though caret is providing a wrapper to nnet(), it's totally possible to get slightly different results for the same problem using caret vs. nnet().  The differences are probably attributable to minor differences in the way certain stuff is calculated and probably not a big deal...but maybe their a huge deal, I haven't thought super hard about it.



